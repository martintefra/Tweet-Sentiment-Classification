{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352de6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "#from implementations import *\n",
    "#from split_data import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bd0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teframartin/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d00539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Main external library : Natural Language Toolkit (nltk)\n",
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8ff2d",
   "metadata": {},
   "source": [
    "### Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b670236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/teframartin/Informatik/ML/project2\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "\n",
    "POSITIVE_DATASET = DATA_FOLDER+\"train_pos.txt\"\n",
    "NEGATIVE_DATASET = DATA_FOLDER+\"train_neg.txt\"\n",
    "\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory) \n",
    "\n",
    "# Note: it seems that the data is already to lower case, so no need to apply lower() to the text\n",
    "pos_data = pd.read_fwf('data/train_pos.txt', header=None, names=[\"text\"]).drop_duplicates()\n",
    "pos_data[\"labels\"] = 1\n",
    "neg_data = pd.read_fwf('data/train_neg.txt', header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
    "neg_data[\"labels\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0991a11",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e4eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50dae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word that is so common that there is no need to use it in a search\n",
    "ENGLISH_STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Adding few extra stop word\n",
    "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS + ['im', 'dont','dunno', 'cant', ' 2 ', \"'s\", ' u ', ' x ', 'ive', 'user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdd884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the two training sets of positive and negative tweets\n",
    "train_data = pd.concat([pos_data, neg_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e64fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common words used in the set of all tweets\n",
    "def get_most_common_words(txt,limit):\n",
    "    return Counter(txt.split()).most_common()[:limit]\n",
    "\n",
    "# Remove from tweets the punctuation and stop words (= a word that is so common that there is no need to use it in a search.)\n",
    "def clean_tweet(tweet):\n",
    "    tweet = \"\".join([w for w in tweet if w not in string.punctuation])\n",
    "    tokens = re.split('\\W+', tweet)\n",
    "    tweet = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    return tweet\n",
    "\n",
    "# Change any word belonging to the same word-family into a common word (changing/changes/changed.. ==> change)\n",
    "def lemmatization(token_tweet):\n",
    "    tweet = [wn.lemmatize(word) for word in token_tweet]\n",
    "    return tweet\n",
    "\n",
    "# Concatenate the tokennized tweet into a all text like at the beginning\n",
    "def concatenate(lst):\n",
    "    concatenate_tweet = ''\n",
    "    for elem in lst:\n",
    "        concatenate_tweet = concatenate_tweet + ' ' + elem\n",
    "    return concatenate_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_tweet transformation\n",
    "train_data['text'] = train_data['text'].apply(lambda x : clean_tweet(x)).apply(lambda x : lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb163b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160157</th>\n",
       "      <td>[readymade, tree, medium, green, 751, 25, 8, assembly, required, readytoplant, tree, lend, authe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20485</th>\n",
       "      <td>[aw, thanks, late, though, final, stage, huge, tooth, makeover, last, appointment, scheduled, to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176511</th>\n",
       "      <td>[17x25, custom, picture, frame, poster, frame, 125, wide, complete, rich, brown, wood, frame, 52...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170994</th>\n",
       "      <td>[peanut, butter, really, visit, leaf, thatd, ahhhmazing, ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>[dress, pretty]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73635</th>\n",
       "      <td>[think, emily, make, sexy, guy, ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>[haha, ill, picture, itll, help, calm, nerve, thanks, pizz, ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162225</th>\n",
       "      <td>[, amazonbasics, usb, 20, amale, afemale, extension, cable, 98, foot, 30, meter, url, wby]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165317</th>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134867</th>\n",
       "      <td>[, hed, get, enjoy, outdoors, lol, unfortunate, coincidence, would, gladly, put, ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       text  \\\n",
       "160157  [readymade, tree, medium, green, 751, 25, 8, assembly, required, readytoplant, tree, lend, authe...   \n",
       "20485   [aw, thanks, late, though, final, stage, huge, tooth, makeover, last, appointment, scheduled, to...   \n",
       "176511  [17x25, custom, picture, frame, poster, frame, 125, wide, complete, rich, brown, wood, frame, 52...   \n",
       "170994                                           [peanut, butter, really, visit, leaf, thatd, ahhhmazing, ]   \n",
       "30517                                                                                       [dress, pretty]   \n",
       "73635                                                                     [think, emily, make, sexy, guy, ]   \n",
       "12976                                         [haha, ill, picture, itll, help, calm, nerve, thanks, pizz, ]   \n",
       "162225           [, amazonbasics, usb, 20, amale, afemale, extension, cable, 98, foot, 30, meter, url, wby]   \n",
       "165317                                                                                                   []   \n",
       "134867                   [, hed, get, enjoy, outdoors, lol, unfortunate, coincidence, would, gladly, put, ]   \n",
       "\n",
       "        labels  \n",
       "160157       0  \n",
       "20485        1  \n",
       "176511       0  \n",
       "170994       0  \n",
       "30517        1  \n",
       "73635        1  \n",
       "12976        1  \n",
       "162225       0  \n",
       "165317       0  \n",
       "134867       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9de3c8",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2c9a4",
   "metadata": {},
   "source": [
    "1. Initialize a task-specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48f51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel(\"roberta\", \"roberta-base\", use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27a63d",
   "metadata": {},
   "source": [
    "2. Train the model with train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1bdd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/907 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/907 [00:02<21:10,  1.40s/it]\n",
      "Epochs 0/1. Running Loss:    0.6771:  39%|███▉      | 45/114 [00:53<01:21,  1.19s/it]\n",
      "Epoch 1 of 1:   0%|          | 0/1 [00:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_data\u001b[39m.\u001b[39;49msample(frac\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m), output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutputs/roberta\u001b[39;49m\u001b[39m\"\u001b[39;49m, args\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39moverwrite_output_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mnum_train_epochs\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/simpletransformers/classification/classification_model.py:632\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    624\u001b[0m     train_dataset,\n\u001b[1;32m    625\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    626\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    627\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    630\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 632\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    633\u001b[0m     train_dataloader,\n\u001b[1;32m    634\u001b[0m     output_dir,\n\u001b[1;32m    635\u001b[0m     multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    636\u001b[0m     show_running_loss\u001b[39m=\u001b[39;49mshow_running_loss,\n\u001b[1;32m    637\u001b[0m     eval_df\u001b[39m=\u001b[39;49meval_df,\n\u001b[1;32m    638\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    639\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    640\u001b[0m )\n\u001b[1;32m    642\u001b[0m \u001b[39m# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39m# model_to_save.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m# self.tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/simpletransformers/classification/classification_model.py:942\u001b[0m, in \u001b[0;36mClassificationModel.train\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    941\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    944\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    945\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train_model(train_data.sample(frac=0.005), output_dir=\"outputs/roberta\", args={\"overwrite_output_dir\": True, \"num_train_epochs\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903877d",
   "metadata": {},
   "source": [
    "3. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an other part of the train full dataset and measure the accuracy\n",
    "# with eval_model()\n",
    "result, model_outputs, wrong_predictions = model.eval_model(train_data.sample(frac=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bb6e7",
   "metadata": {},
   "source": [
    "4. Make predictions on (unlabelled) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c031424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file on the test dataset\n",
    "#  with predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23636a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
