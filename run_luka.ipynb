{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "352de6db",
      "metadata": {
        "id": "352de6db"
      },
      "outputs": [],
      "source": [
        "# Useful starting lines\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "#from implementations import *\n",
        "#from split_data import * \n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install simpletransformers"
      ],
      "metadata": {
        "id": "Gi28y7xOPxeC",
        "outputId": "ae113115-a23e-4a9e-ef31-0eea59ff4ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Gi28y7xOPxeC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
            "\u001b[K     |████████████████████████████████| 250 kB 30.6 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.6.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.64.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2022.6.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.3.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Collecting wandb>=0.10.32\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 55.5 MB/s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "02bd0372",
      "metadata": {
        "id": "02bd0372",
        "outputId": "c5e3d7b0-9cb2-4a4f-877e-5906df34d1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b8fe2d63d8a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassificationArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simpletransformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d00539",
      "metadata": {
        "id": "69d00539"
      },
      "outputs": [],
      "source": [
        "# Main external library : Natural Language Toolkit (nltk)\n",
        "import nltk\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f8ff2d",
      "metadata": {
        "id": "06f8ff2d"
      },
      "source": [
        "### Load tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b670236f",
      "metadata": {
        "id": "b670236f"
      },
      "outputs": [],
      "source": [
        "DIRECTORY = \"data/\"\n",
        "\n",
        "# Note: it seems that the data is already to lower case, so no need to apply lower() to the text\n",
        "pos_data = pd.read_fwf(\"/content/train_neg_full.txt\", header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
        "pos_data[\"labels\"] = 1\n",
        "neg_data = pd.read_fwf('/content/train_pos_full.txt', header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
        "neg_data[\"labels\"] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0991a11",
      "metadata": {
        "id": "b0991a11"
      },
      "source": [
        "### Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e4eb29",
      "metadata": {
        "id": "e9e4eb29"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "pd.set_option('display.max_colwidth',100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50dae3f",
      "metadata": {
        "id": "a50dae3f"
      },
      "outputs": [],
      "source": [
        "# A word that is so common that there is no need to use it in a search\n",
        "ENGLISH_STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Adding few extra stop word\n",
        "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS + ['im', 'dont','dunno', 'cant', ' 2 ', \"'s\", ' u ', ' x ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdd884a",
      "metadata": {
        "id": "9cdd884a"
      },
      "outputs": [],
      "source": [
        "#Concatenate the two training sets of positive and negative tweets\n",
        "train_data = pd.concat([pos_data, neg_data], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.sample(frac = 1)\n"
      ],
      "metadata": {
        "id": "YdKM6bhBOKft"
      },
      "id": "YdKM6bhBOKft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "1SBgAvKDMUNu"
      },
      "id": "1SBgAvKDMUNu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e64fc2e",
      "metadata": {
        "id": "5e64fc2e"
      },
      "outputs": [],
      "source": [
        "# Calculate the most common words used in the set of all tweets\n",
        "def get_most_common_words(txt,limit):\n",
        "    return Counter(txt.split()).most_common()[:limit]\n",
        "\n",
        "# Remove from tweets the punctuation and stop words (= a word that is so common that there is no need to use it in a search.)\n",
        "def clean_tweet(tweet):\n",
        "    tweet = \"\".join([w for w in tweet if w not in string.punctuation])\n",
        "    tokens = re.split('\\W+', tweet)\n",
        "    tweet = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
        "    return tweet\n",
        "\n",
        "# Change any word belonging to the same word-family into a common word (changing/changes/changed.. ==> change)\n",
        "def lemmatization(token_tweet):\n",
        "    tweet = [wn.lemmatize(word) for word in token_tweet]\n",
        "    return tweet\n",
        "\n",
        "# Concatenate the tokennized tweet into a all text like at the beginning\n",
        "def concatenate(lst):\n",
        "    concatenate_tweet = ''\n",
        "    for elem in lst:\n",
        "        concatenate_tweet = concatenate_tweet + ' ' + elem\n",
        "    return concatenate_tweet\n",
        "#we should remove also the numeber appeared in the tweet since doesn't contains any information "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07068f1",
      "metadata": {
        "id": "d07068f1"
      },
      "outputs": [],
      "source": [
        "# Apply the clean_tweet transformation\n",
        "train_data['text'] = train_data['text'].apply(lambda x : clean_tweet(x)).apply(lambda x : lemmatization(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "LpLHcjBUMaGL"
      },
      "id": "LpLHcjBUMaGL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0d9de3c8",
      "metadata": {
        "id": "0d9de3c8"
      },
      "source": [
        "### Creation of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed2c9a4",
      "metadata": {
        "id": "4ed2c9a4"
      },
      "source": [
        "1. Initialize a task-specific model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48f51c2",
      "metadata": {
        "id": "a48f51c2"
      },
      "outputs": [],
      "source": [
        "model = ClassificationModel(\"roberta\", \"roberta-base\", use_cuda=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef27a63d",
      "metadata": {
        "id": "ef27a63d"
      },
      "source": [
        "2. Train the model with train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1bdd71",
      "metadata": {
        "id": "5c1bdd71"
      },
      "outputs": [],
      "source": [
        "model.train_model(train_data.sample(frac=0.005), output_dir=\"outputs/roberta\", args={\"overwrite_output_dir\": True, \"num_train_epochs\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9903877d",
      "metadata": {
        "id": "9903877d"
      },
      "source": [
        "3. Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cbfa17",
      "metadata": {
        "id": "58cbfa17"
      },
      "outputs": [],
      "source": [
        "# sample an other part of the train full dataset and measure the accuracy\n",
        "# with eval_model()\n",
        "result, model_outputs, wrong_predictions = model.eval_model(train_data.sample(frac=0.005))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5e31b1",
      "metadata": {
        "id": "5b5e31b1"
      },
      "outputs": [],
      "source": [
        "result, model_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083bb6e7",
      "metadata": {
        "id": "083bb6e7"
      },
      "source": [
        "4. Make predictions on (unlabelled) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c031424",
      "metadata": {
        "id": "5c031424"
      },
      "outputs": [],
      "source": [
        "# create the submission file on the test dataset\n",
        "#  with predict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfZ2S0ilJGzq"
      },
      "id": "MfZ2S0ilJGzq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfDBuH3vJG3D"
      },
      "id": "vfDBuH3vJG3D",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ebc07cba07d33741e40e0d779bcc934485fb1a6cfaaf5d05a7b4d79b8578fbe8"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}