{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352de6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "#from implementations import *\n",
    "#from split_data import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bd0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d00539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Main external library : Natural Language Toolkit (nltk)\n",
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8ff2d",
   "metadata": {},
   "source": [
    "### Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b670236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/teframartin/Informatik/ML/project2/notebooks\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"../data/\"\n",
    "\n",
    "POSITIVE_DATASET = DATA_FOLDER+\"train_pos.txt\"\n",
    "NEGATIVE_DATASET = DATA_FOLDER+\"train_neg.txt\"\n",
    "\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory) \n",
    "\n",
    "# Note: it seems that the data is already to lower case, so no need to apply lower() to the text\n",
    "pos_data = pd.read_fwf(POSITIVE_DATASET, header=None, names=[\"text\"]).drop_duplicates()\n",
    "pos_data[\"labels\"] = 1\n",
    "neg_data = pd.read_fwf(NEGATIVE_DATASET, header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
    "neg_data[\"labels\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0991a11",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e4eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a50dae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word that is so common that there is no need to use it in a search\n",
    "ENGLISH_STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Adding few extra stop word\n",
    "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS + ['im', 'dont','dunno', 'cant', ' 2 ', \"'s\", ' u ', ' x ', 'ive', 'user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdd884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the two training sets of positive and negative tweets\n",
    "train_data = pd.concat([pos_data, neg_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e64fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common words used in the set of all tweets\n",
    "def get_most_common_words(txt,limit):\n",
    "    return Counter(txt.split()).most_common()[:limit]\n",
    "\n",
    "# Remove from tweets the punctuation and stop words (= a word that is so common that there is no need to use it in a search.)\n",
    "def clean_tweet(tweet):\n",
    "    tweet = \"\".join([w for w in tweet if w not in string.punctuation])\n",
    "    tokens = re.split('\\W+', tweet)\n",
    "    tweet = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    return tweet\n",
    "\n",
    "# Change any word belonging to the same word-family into a common word (changing/changes/changed.. ==> change)\n",
    "def lemmatization(token_tweet):\n",
    "    tweet = [wn.lemmatize(word) for word in token_tweet]\n",
    "    return tweet\n",
    "\n",
    "# Concatenate the tokennized tweet into a all text like at the beginning\n",
    "def concatenate(lst):\n",
    "    concatenate_tweet = ''\n",
    "    for elem in lst:\n",
    "        concatenate_tweet = concatenate_tweet + ' ' + elem\n",
    "    return concatenate_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_tweet transformation\n",
    "train_data['text'] = train_data['text'].apply(lambda x : clean_tweet(x)).apply(lambda x : lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb163b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122679</th>\n",
       "      <td>[deer, noctural, around, deer, humorous, look, deer, herd, frequent, property, url]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115851</th>\n",
       "      <td>[creation, death, mass, market, paperback, 25th, death, novel, 1, new, york, time, bestselling, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38548</th>\n",
       "      <td>[love, god, joe, please, try, bit, careful, would, like, one, bit, upcoming, gig, please, x, lol]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12254</th>\n",
       "      <td>[got, bit, thing, sb, lemon, poppy, seed, skinny, muffin, latte, think, better, way, start, day, ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147535</th>\n",
       "      <td>[mlb, padre, infof, blank, seasonending, surgery, yahoo, sport, san, diego, ap, padre, infi, url...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32687</th>\n",
       "      <td>[loll, wait, till, guy, done, waiting, till, guy, finish, till, shall, relax, ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40547</th>\n",
       "      <td>[awesome, looking, forward]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126956</th>\n",
       "      <td>[white, knotted, baby, hat, rose, pink, double, bow, 36, month, 16, pound, adorable, soft, inter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68061</th>\n",
       "      <td>[aha, aw, thanks, cutie]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103344</th>\n",
       "      <td>[sorry, today, practice, long, came, home, tired, sorrry, guess, boy, anymore]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       text  \\\n",
       "122679                  [deer, noctural, around, deer, humorous, look, deer, herd, frequent, property, url]   \n",
       "115851  [creation, death, mass, market, paperback, 25th, death, novel, 1, new, york, time, bestselling, ...   \n",
       "38548     [love, god, joe, please, try, bit, careful, would, like, one, bit, upcoming, gig, please, x, lol]   \n",
       "12254    [got, bit, thing, sb, lemon, poppy, seed, skinny, muffin, latte, think, better, way, start, day, ]   \n",
       "147535  [mlb, padre, infof, blank, seasonending, surgery, yahoo, sport, san, diego, ap, padre, infi, url...   \n",
       "32687                       [loll, wait, till, guy, done, waiting, till, guy, finish, till, shall, relax, ]   \n",
       "40547                                                                           [awesome, looking, forward]   \n",
       "126956  [white, knotted, baby, hat, rose, pink, double, bow, 36, month, 16, pound, adorable, soft, inter...   \n",
       "68061                                                                              [aha, aw, thanks, cutie]   \n",
       "103344                       [sorry, today, practice, long, came, home, tired, sorrry, guess, boy, anymore]   \n",
       "\n",
       "        labels  \n",
       "122679       0  \n",
       "115851       0  \n",
       "38548        1  \n",
       "12254        1  \n",
       "147535       0  \n",
       "32687        1  \n",
       "40547        1  \n",
       "126956       0  \n",
       "68061        1  \n",
       "103344       0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9de3c8",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2c9a4",
   "metadata": {},
   "source": [
    "1. Initialize a task-specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel(\"roberta\", \"roberta-base\", use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27a63d",
   "metadata": {},
   "source": [
    "2. Train the model with train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1bdd71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mtrain_model(train_data\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m), output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moutputs/roberta\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39moverwrite_output_dir\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mnum_train_epochs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.train_model(train_data.sample(frac=0.005), output_dir=\"outputs/roberta\", args={\"overwrite_output_dir\": True, \"num_train_epochs\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903877d",
   "metadata": {},
   "source": [
    "3. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an other part of the train full dataset and measure the accuracy\n",
    "# with eval_model()\n",
    "result, model_outputs, wrong_predictions = model.eval_model(train_data.sample(frac=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bb6e7",
   "metadata": {},
   "source": [
    "4. Make predictions on (unlabelled) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c031424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file on the test dataset\n",
    "#  with predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23636a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
