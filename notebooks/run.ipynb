{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing + pretrained GloVe embeddings + 6 LSTM models + XGboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blZBSwYTxxw8",
    "outputId": "5ba8b387-07db-4933-9060-13066ad917d5"
   },
   "outputs": [],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkUbJseFxIVY",
    "outputId": "cfccc543-311b-4702-bb97-ef9f0347b463"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1PBpF-mxZK5",
    "outputId": "4af185fe-967a-44ea-b949-ca0a39ad656a"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1sedgl7V0Sm",
    "outputId": "5ecf19c2-e0c0-49cc-cbb4-4753913129bd"
   },
   "outputs": [],
   "source": [
    "pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQGqF6RzuiPx",
    "outputId": "c211571a-e083-467d-94e8-34c3542c880e"
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tdt8xkCrJNNe",
    "outputId": "0477d27b-c45c-431c-ada2-5b546ab4d838"
   },
   "outputs": [],
   "source": [
    "#!apt install -qq enchant #if it's necessary\n",
    "!pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DYM2Ld_8e3O",
    "outputId": "5e033265-6014-448a-fc4d-1875d07a829a"
   },
   "outputs": [],
   "source": [
    "!pip install emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgUumJ1aPMKC"
   },
   "outputs": [],
   "source": [
    "#Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "\n",
    "#Sklearn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Build the LSTM model\n",
    "import tensorflow as tf\n",
    "import pickle as cPickle\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import  Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "#for expansion the contractions for instance I'll or I've been to I will and I have been and not only \n",
    "import contractions \n",
    "\n",
    "#import emoticons in order to replace them with appropriate words\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n",
    "\n",
    "#read html files \n",
    "import requests\n",
    "\n",
    "#split the \n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSK5ojfZuskX"
   },
   "outputs": [],
   "source": [
    "\n",
    "url_positive = \"https://ptrckprry.com/course/ssd/data/positive-words.txt\"\n",
    "rsp = requests.get(url_positive)\n",
    "lines = rsp.text.strip(\"\\n\").split(\"\\n\")\n",
    "positive_words = lines[lines.index('a+'):]\n",
    "\n",
    "url_negative = \"https://ptrckprry.com/course/ssd/data/negative-words.txt\"\n",
    "rsp = requests.get(url_negative)\n",
    "lines = rsp.text.strip(\"\\n\").split(\"\\n\")\n",
    "negative_words = lines[lines.index('2-faced'):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZpQPYuYuKcs"
   },
   "outputs": [],
   "source": [
    "def expansion_patterns(text):\n",
    "\n",
    "    expansion_patterns = [(' nd ',' and '),(' wa ',' was '),(' donnow ',' do not know '),(' i\\'ts ','it is '),\n",
    "                      (' dem ',' them '),(' #+ha+ha ',' haha '),(' i\\'ts ','it is '),(' i\\'ts ','it is '),(' n+a+h+ ', ' no '),\n",
    "                      (' n+a+ ', ' no '),(' w+o+w+', 'wow '),('y+a+y+', 'yay'),('y+[e,a]+s+', 'yes'),\n",
    "                      (' ya ', ' you '),('n+o+', 'no'),('a+h+','ah'),('muah','kiss'),(' y+u+p+ ', ' yes '),(' y+e+p+ ', ' yes '),\n",
    "                      (' ima ', ' i am going to '),(' woah ', ' wow '),(' wo ', ' wow '),(' aw ', ' cute '), \n",
    "                      (' lmao ', ' haha '),(' lol ', ' haha ')]\n",
    "    patterns = [(re.compile(regex_exp, re.IGNORECASE), replacement) for (regex_exp, replacement) in expansion_patterns]\n",
    "    for (pattern, replacement) in patterns:\n",
    "        (text, _) = re.subn(pattern, replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEuuxE6IunIk"
   },
   "outputs": [],
   "source": [
    "\n",
    "#A more robust preprocessing phase \n",
    "#import the stopword list from the spacy library \n",
    " \n",
    "EMOTICONS_EMO[':d'] = 'laughing '\n",
    "EMOTICONS_EMO['<3'] = 'red heart'\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "stopwords_dict = Counter(spacy_stopwords)\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "\n",
    "COUNT = 0\n",
    "\n",
    "def increment():\n",
    "    global COUNT\n",
    "    COUNT = COUNT+1\n",
    "\n",
    "\n",
    "#cleaning \"pipeline\"  \n",
    "def clean_data(text, stopwords, lemmatization):\n",
    "\n",
    "      #TODO\n",
    "      #map the emoji to lexicon \n",
    "      #for instance if we have 'text text <3 :d text :D' will be   'text text red heart positive laughing positive text Laughing'\n",
    "      text = ' '.join(EMOTICONS_EMO.get(word) if word in EMOTICONS_EMO.keys() else word for word in text.split() ) \n",
    "      #print('emoji done')\n",
    "\n",
    "      #perform casefolding\n",
    "      text =text.casefold()\n",
    "      #print('casefold done')\n",
    "\n",
    "      #remove punctuations for each twitter\n",
    "      text = ' '.join(text_ for text_ in text.split() if text_ not in string.punctuation)\n",
    "      #print('punctuations done')\n",
    "\n",
    "      #remove all numbers not just digits since doesn't give so much information for the purpose of sentimental analysis\n",
    "      #for instance '#5words 625' with be '#words and then will be \"words\" after removing the hashtags in the later phase of preprocessing\n",
    "      \n",
    "      text = ' '.join(re.sub('(\\d+(\\.\\d+)?)','',word) if re.search('(\\d+(\\.\\d+)?)',word) else word for word in text.split() ).strip()\n",
    "      #print('remove numbers done')\n",
    "\n",
    "      #remove different tags for instance \"<user>,<url>\" for each twitter\n",
    "      text = re.sub('<[^<]+?>','', text)\n",
    "      #print('remove different tags done')\n",
    "\n",
    "      #remove multiply commas and dots everywhere in tweets      \n",
    "      text = re.sub('\\.|,*','', text)\n",
    "      #print('remove multiply commas done')\n",
    "\n",
    "      #expansion patterns\n",
    "      text=expansion_patterns(text)\n",
    "      #print('remove expansion patterns')\n",
    "\n",
    "      #contractions from the library \"coz\"(because) is too powerful for instance coz to because or I'll to I will or don't to do not or even dont to do not \n",
    "      text = ' '.join(contractions.fix(text_) for text_ in text.split() ) \n",
    "      #print('contractions done')\n",
    "\n",
    "      if stopwords:\n",
    "          #remove the stopwords\n",
    "          text = ' '.join([word for word in text.split() if word not in stopwords_dict])         \n",
    "      #print('stopwords done')\n",
    "      \n",
    "      #split th words within a hashtags , if it's unsplittable we will remove the word i.e if #happythoughts with be happy thoughts using the library compound word splitter \n",
    "      #for instance the tweet \n",
    "      #<user> hahahhahaha aw dont cry #thinkhappythoughts .. yeah right #cryyoureffingeyesout will be\n",
    "      #<user> hahahhahaha aw dont cry think happy thoughts .. yeah right cry your effing eyes out\n",
    "      \n",
    "      text=' '.join( ' '.join(wordninja.split(word_[1:])) if word_.startswith('#') else word_ for word_ in text.split() )\n",
    "      #print('split hashtag done')\n",
    "\n",
    "      if lemmatization :\n",
    "          #perform lemmatization\n",
    "          text = ' '.join(lemmatizer.lemmatize(text_)  for text_ in text.split() )\n",
    "      #print('lemmatization done')\n",
    "\n",
    "      # use the positive and negative sentimental analysis adding the appropriate words in each tweet using predefined dictionaries/vocabularies\n",
    "      #https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets we used the list of postive and negative lexicon word to add postive and negative token to our tweets\n",
    "      \n",
    "      text = ' '.join(word+\" positive\" if word in positive_words else word for word in text.split() )\n",
    "      text = ' '.join(word+\" negative\" if word in negative_words else word for word in text.split() )\n",
    "      #print('postive done')\n",
    "\n",
    "      #remove the tokens length less than 2 again if some may appear after the above preprocessing\n",
    "      text = ' '.join(text_ for text_ in text.split() if len(text_)>2)\n",
    "      #print('#remove the tokens done')\n",
    "\n",
    "\n",
    "      increment()\n",
    "      if(COUNT%10000==0):\n",
    "        print(COUNT)\n",
    "      return  text.strip()\n",
    "\n",
    "\n",
    "#Load the data and run the preprocessor pipeline \n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Init function\n",
    "        \"\"\"\n",
    "    def load_data(preprocessed=True,Train_data=True):\n",
    "        DIRECTORY1 = \"../data/train_pos.txt\"\n",
    "        DIRECTORY2 = \"../data/train_neg.txt\"\n",
    "\n",
    "        \n",
    "        #import the data\n",
    "\n",
    "        if Train_data==True:\n",
    "          pos_data = pd.read_fwf(DIRECTORY1, header=None, names=[\"tweets\"])\n",
    "          pos_data[\"label\"] = 1.0\n",
    "\n",
    "          neg_data = pd.read_fwf(DIRECTORY2, header=None, names=[\"tweets\"])\n",
    "          neg_data[\"label\"] = 0.0\n",
    "          data = pd.concat([pos_data, neg_data], ignore_index=True)\n",
    "          np.random.seed(500)\n",
    "          #shuffle the merge data\n",
    "          data = data.iloc[np.random.permutation(len(data))]\n",
    "\n",
    "          #print(pos_data.isnull().any(axis=1))\n",
    "        else:\n",
    "          with open('../data/test_data.txt') as f:\n",
    "            data = f.readlines()\n",
    "          data = pd.DataFrame(data,columns=['tweets'])\n",
    "          data.tweets=data.tweets.apply(lambda x :x[x.find(',')+1:])\n",
    "        #data.dropna(subset = [\"tweets\"], inplace=True)\n",
    "        data['tweets']=data['tweets'].apply(lambda x : clean_data(x, stopwords=True,lemmatization=True))\n",
    "        \n",
    "        #remove empty lines if any  \n",
    "        #data.dropna(subset = [\"tweets\"], inplace=True)\n",
    "\n",
    "        #X = data['tweets'].values\n",
    "        #y = np.stack((data['positive'],data['negative']),axis=-1)\n",
    "\n",
    "        return data#np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-1Xv7RiRzDc",
    "outputId": "fbca09a2-bc00-487d-8e86-dfc2c4581151"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = Preprocessor.load_data(preprocessed=True,Train_data=False)\n",
    "#it takes ~1 minute to run\n",
    "\n",
    "\n",
    "#powerful preprocessing\n",
    "#,... currently workn out ... <user> park ) #anycompany ? \n",
    "#currently workn park company\n",
    "\n",
    "#OR\n",
    "#3417,<user> loool 7yaaatii mbyn alejtehaad hhh :p p mnn jddd <3 3 , abshrk ana b3d praise is due to allah =) ) :p p\n",
    "#loool yaaatii mbyn alejtehaad hhh mnn jddd red heart positive positive abshrk ana praise positive allah happy positive face smiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "haUj7IyoZI9t",
    "outputId": "1e8a7d16-f7b9-433c-abf7-259165fd40dc"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMkrVCGy5KYt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_full = Preprocessor.load_data(preprocessed=True,Train_data=True)\n",
    "#it takes ~2 hour to complete the preprocessing phase in the entire 2.5M tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eEdVvAlfvaW"
   },
   "outputs": [],
   "source": [
    "#so we run it in two different accounts in colab and then combine the two preprocessed sets \n",
    "#X_positive = pd.read_csv('/content/drive/MyDrive/X_positive_preprocessed.csv')\n",
    "#X_negative = pd.read_csv('/content/drive/MyDrive/X_negative_preprocessed.csv')\n",
    "#X_full_preprocessed = pd.concat([X_positive, X_negative], ignore_index=True)\n",
    "#np.random.seed(500)\n",
    "#shuffle the merge data\n",
    "#X_full_preprocessed = X_full_preprocessed.iloc[np.random.permutation(len(X_full_preprocessed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcn5s8SC5Zw7"
   },
   "outputs": [],
   "source": [
    "#X_full_preprocessed.tweets=X_full_preprocessed.tweets.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3oDZjx8FhhM-",
    "outputId": "b4617187-19eb-4edb-e172-2b4235a34f39"
   },
   "outputs": [],
   "source": [
    "X_full_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptDRhrydurrz"
   },
   "outputs": [],
   "source": [
    "y_train=X_full_preprocessed.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K72koNKUuzFs"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(X_full_preprocessed.tweets)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_full_preprocessed.tweets)#convert each word to a integer based on the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "724ZylTD5nrr"
   },
   "outputs": [],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1 \n",
    "max_len=40 # maxh length of each tweet is set to 40 words s(so will be performed padding and truncation )\n",
    "X_train = pad_sequences(X_train, padding='post'  ,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zURKpFWzxR2k"
   },
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test.tweets)#covert each word to a integer based on the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xldwGy8xMBg"
   },
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, padding='post'  ,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_XolwCXyfkD",
    "outputId": "66ac05f9-11d2-4c79-dcde-8dc6f13084d7"
   },
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivcb5RdAyZx1",
    "outputId": "e7f6de5d-341e-4128-b0cd-c7f8a11f860c"
   },
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w4sRJEU5JxD"
   },
   "source": [
    "**USE THE PRETRAINED GLVOE EMBEDDINGS FOR OUR TEXT TO VECTOR REPRESENTATIONS SINCE IS MORE POWERFUL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5YhC7jTuGhK"
   },
   "outputs": [],
   "source": [
    "\n",
    "#retrieve the pretrained embeddings and store them as a dictionary\n",
    "embeddings_index = {}\n",
    "#f = open('/content/drive/MyDrive/glove.twitter.27B.200d.txt', encoding='utf-8')\n",
    "f = open('../data/glove.twitter.27B.200d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsJmQtae0uxo"
   },
   "outputs": [],
   "source": [
    "#embeddings_index.get('luka') # for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh-Z5pMIwK-Y",
    "outputId": "85bb1fba-8390-4b16-9fda-00a021b07a10"
   },
   "outputs": [],
   "source": [
    "\n",
    "#form our embedding matrix for each word that appears in our dataset based on pretrained glove embeddings\n",
    "embedding_matrix = np.zeros((vocab_size , 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)\n",
    "#cPickle.dump([embedding_matrix],open('/content/drive/MyDrive/embedding_matrix_full_preprocessing.dat', 'wb'))#write the embedding_matrix in file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J22Zlr8c7A_",
    "outputId": "c424f3e1-87c4-4262-d4be-664b1ef458b2"
   },
   "outputs": [],
   "source": [
    "#embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RV0hBFLzYNYC"
   },
   "source": [
    "**TRAIN A GLOVE MODEL FOR THE EMBEDDINGS ON OUR CORPUS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "GKsAFzld3gYA",
    "outputId": "6fbf742e-d5f1-4568-faf9-410c0a434a37"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# instantiate the corpus\n",
    "corpus = Corpus() \n",
    "# this will create the word co occurence matrix \n",
    "corpus.fit(X_200.tweets, window=1000)\n",
    "\n",
    "# instantiate the model\n",
    "glove_model = Glove(no_components=200, learning_rate=0.1)\n",
    "\n",
    "# and fit over the corpus matrix\n",
    "glove_model.fit(corpus.matrix, epochs=20, no_threads=32)\n",
    "\n",
    "# finally we add the vocabulary to the model\n",
    "glove_model.add_dictionary(corpus.dictionary)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cq-mqyWTAgHW"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#form our embedding matrix for each word that appears in our dataset based on trained glove embeddings on our corpus\n",
    "embedding_matrix = np.zeros((max_features + 1, 200))\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = glove_model.word_vectors[glove_model.dictionary[word]]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "           \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCyQ_YWiY7mt"
   },
   "source": [
    "**DIFFERENT NEURAL NETWORK MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpuevmvjOnLL"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.layers import Embedding, SpatialDropout1D , Conv1D\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Dropout,Masking,Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam  # SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zk_Qa8ujZFqB",
    "outputId": "6efefd92-ea13-4ae7-fb70-be4b913e7ed6"
   },
   "outputs": [],
   "source": [
    "#MODEL 1 \n",
    "#vocab_length=max_features+1\n",
    "embedding_size=200\n",
    "num_of_words=40 # train_sequences.shape[1]\n",
    "\n",
    "#first model : simple neural network\n",
    "model1 = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False,mask_zero=True) #trainable set to False bc we use the downloaded dict\n",
    "model1.add(embedding_layer)\n",
    "model1.add(Masking(mask_value=0.0)) #need masking layer to not train on padding (so for that words whicha weren't in the pretrained glove embeddings so their representation is full of zeros)\n",
    "model1.add(Bidirectional(LSTM(512)))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxv_EfbsDnYb"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7hTuE8tgx_2",
    "outputId": "322ec112-7bdf-41a0-ee2a-883a5a5850a2"
   },
   "outputs": [],
   "source": [
    "EPOCHS=6\n",
    "BATCH_SIZE=1024\n",
    "\n",
    "model1.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
    "#train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n60DU9KOy3b3"
   },
   "outputs": [],
   "source": [
    "#model1.save('/content/drive/MyDrive/model_1',save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic2QF7XPa7RE"
   },
   "source": [
    "**MODEL 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbFk6-Ok-ITF",
    "outputId": "0bacb388-7aab-43ea-da52-076475a18b80"
   },
   "outputs": [],
   "source": [
    "#MODEL 2 # has potential for more accurate predictions\n",
    "#vocab_length=max_features+1\n",
    "embedding_size=200 \n",
    "num_of_words=40 # train_sequences.shape[1]\n",
    "\n",
    "#first model : simple neural network\n",
    "model2 = Sequential()\n",
    "embedding_layer2 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False) #trainable set to False bc we use the downloaded dict\n",
    "model2.add(embedding_layer2)\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(64))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8g9xXNp-OId"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model2, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEgSyAmv_umx",
    "outputId": "2368842f-cfb6-488f-e28c-4505edb2f800"
   },
   "outputs": [],
   "source": [
    "EPOCHS=6\n",
    "BATCH_SIZE=128\n",
    "\n",
    "model2.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
    "#train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6AHlH3Uy7TW"
   },
   "outputs": [],
   "source": [
    "#model2.save('/content/drive/MyDrive/model_2',save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoNIxQ08bDyk"
   },
   "source": [
    "**MODEL 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24S2Y0zBGvsS"
   },
   "outputs": [],
   "source": [
    "#MODEL 3\n",
    "#vocab_length=max_features+1\n",
    "embedding_size=200\n",
    "num_of_words=40 # train_sequences.shape[1]\n",
    "\n",
    "#first model : simple neural network\n",
    "model3 = Sequential()\n",
    "embedding_layer3 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False) #trainable set to False bc we use the downloaded dict\n",
    "model3.add(embedding_layer3)\n",
    "model3.add(LSTM(1024))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(512,activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(512,activation='relu'))\n",
    "\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3hR0NAdeeKE"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model3, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhmaSFQyHuvy"
   },
   "outputs": [],
   "source": [
    "EPOCHS=6\n",
    "BATCH_SIZE=128\n",
    "\n",
    "model3.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
    "#train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j6j0SHIzAW8"
   },
   "outputs": [],
   "source": [
    "#model3.save('/content/drive/MyDrive/model_3',save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwSiNcsFbLOI"
   },
   "source": [
    "**MODEL 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spCudgLEatPV"
   },
   "outputs": [],
   "source": [
    "#MODEL 4\n",
    "\n",
    "#vocab_length=max_features+1\n",
    "embedding_size=200\n",
    "num_of_words=40 # train_sequences.shape[1]\n",
    "\n",
    "#first model : simple neural network\n",
    "model4 = Sequential()\n",
    "embedding_layer2 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False,mask_zero=True) #trainable set to False bc we use the downloaded dict\n",
    "model4.add(embedding_layer2)\n",
    "model4.add(Masking(mask_value=0.0)) #need masking layer to not train on padding (so for that words whicha weren't in the pretrained glove embeddings so their representation is full of zeros)\n",
    "model4.add(LSTM(512,return_sequences=True))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(LSTM(512,return_sequences=True))\n",
    "model4.add(LSTM(265))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(1))\n",
    "model4.add(Activation('sigmoid'))\n",
    "\n",
    "model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCyEiWwYayhG"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model4, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcPUABRta1vi"
   },
   "outputs": [],
   "source": [
    "EPOCHS=6\n",
    "BATCH_SIZE=1024\n",
    "model4.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQl1XCOTzHKA"
   },
   "outputs": [],
   "source": [
    "#model4.save('/content/drive/MyDrive/model_4',save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpS1svSUcyER"
   },
   "source": [
    "**MODEL 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDd33TE7dIJn"
   },
   "outputs": [],
   "source": [
    "\n",
    "#MODEL 5\n",
    "#vocab_length=max_features+1\n",
    "#embedding_size=200\n",
    "num_of_words=40 # train_sequences.shape[1]\n",
    "embedding_size=200\n",
    "#first model : simple neural network\n",
    "model5 = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=num_of_words , trainable=False,mask_zero=True) #trainable set to False bc we use the downloaded dict\n",
    "model5.add(embedding_layer)\n",
    "model5.add(Masking(mask_value=0.0)) #need masking layer to not train on padding (so for that words whicha weren't in the pretrained glove embeddings so their representation is full of zeros)\n",
    "model5.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model5.add(MaxPooling1D(pool_size=2))\n",
    "model5.add(LSTM(256))\n",
    "model5.add(Dense(64, activation='relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(1))\n",
    "model5.add(Activation('sigmoid'))\n",
    "\n",
    "model5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDUkKOcldVSA"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model5, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHk2M7y1ddc7"
   },
   "outputs": [],
   "source": [
    "EPOCHS=6\n",
    "BATCH_SIZE=1024\n",
    "model5.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
    "#train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ipn6rsfuzKcb"
   },
   "outputs": [],
   "source": [
    "#model5.save('/content/drive/MyDrive/model_5',save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnLVSaLcdmvM"
   },
   "source": [
    "**MODEL 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjV1xEAEd0Wj"
   },
   "outputs": [],
   "source": [
    "\n",
    "#MODEL 6 \n",
    "\n",
    "#vocab_length=max_features+1\n",
    "embedding_size=200\n",
    "num_of_words=40 # train_sequences.shape[1]\n",
    "\n",
    "#first model : simple neural network\n",
    "model6 = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False) #trainable set to False bc we use the downloaded dict\n",
    "model6.add(embedding_layer)\n",
    "model6.add(Bidirectional(LSTM(1024)))\n",
    "model6.add(Dense(512, activation='relu'))\n",
    "model6.add(Dropout(0.4))\n",
    "model6.add(Dense(512, activation='relu'))\n",
    "model6.add(Dropout(0.4))\n",
    "model6.add(Dense(512, activation='relu'))\n",
    "model6.add(Dropout(0.4))\n",
    "model6.add(Dense(1))\n",
    "model6.add(Activation('sigmoid'))\n",
    "\n",
    "model6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WD0_gxZmd5iA"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model6, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQdazrg7d9wh"
   },
   "outputs": [],
   "source": [
    "EPOCHS=6\n",
    "BATCH_SIZE=1024\n",
    "\n",
    "#train the model\n",
    "model6.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLyTdE2ZzNqS"
   },
   "outputs": [],
   "source": [
    "#model6.save('/content/drive/MyDrive/model_6',save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2ER21UYgMxk"
   },
   "outputs": [],
   "source": [
    "#retrive the training predictions and testing predictions in order to combine these results using XGb classifier\n",
    "train1 = model1.predict(X_train, batch_size=128)\n",
    "test1 = model1.predict(X_test)\n",
    "\n",
    "train2 = model2.predict(X_train, batch_size=128)\n",
    "test2 = model2.predict(X_test)\n",
    "\n",
    "train3 = model3.predict(X_train, batch_size=128)\n",
    "test3 = model3.predict(X_test)\n",
    "\n",
    "train4 = model4.predict(X_train, batch_size=128)\n",
    "test4 = model4.predict(X_test)\n",
    "\n",
    "train5 = model5.predict(X_train, batch_size=128)\n",
    "test5 = model5.predict(X_test)\n",
    "\n",
    "train6 = model6.predict(X_train, batch_size=128)\n",
    "test6 = model6.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwjfmEKyiOfM"
   },
   "outputs": [],
   "source": [
    "#combine all the training predictions and testing predictions in order to train a XBGbooster for more accuracy\n",
    "train = np.hstack((train1, train2, train3, train4, train5))\n",
    "test = np.hstack((test1, test2, test3, test4, test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZUMa1WhiwxB"
   },
   "outputs": [],
   "source": [
    "\n",
    "#train a XGB booster\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier().fit(train, y_train)\n",
    "y_pred = model.predict(test)\n",
    "y_pred=[-1 if y_p<0.5 else 1 for y_p in y_pred ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHSSs74yxyi5"
   },
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in range(1,10001):\n",
    "  index.append(i)\n",
    "index=pd.DataFrame(index,columns=['Id'])\n",
    "predictions=pd.DataFrame(y_pred,columns=[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K46L0bLmx3-Y"
   },
   "outputs": [],
   "source": [
    "predictions_final=pd.concat([index, predictions], ignore_index=False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqtCuE7sx7Kp"
   },
   "outputs": [],
   "source": [
    "predictions_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXfP0zOEx6Mf"
   },
   "outputs": [],
   "source": [
    "predictions_final.to_csv('/submission_last.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
