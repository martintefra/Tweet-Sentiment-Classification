{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from numpy import loadtxt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word that is so common that there is no need to use it in a search\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# Adding few extra stop word\n",
    "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS + ['im', 'dont','dunno', 'cant',\"'s\", 'u', 'x','user','url','rt','lol', '<user>', '<url>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common words used in the set of all tweets\n",
    "def get_most_common_words(txt,limit):\n",
    "    return Counter(txt.split()).most_common()[:limit]\n",
    "\n",
    "# Remove from tweets the punctuation and stop words (= a word that is so common that there is no need to use it in a search.)\n",
    "def clean_tweet(tweet):\n",
    "    tweet = \"\".join([w for w in tweet if w not in string.punctuation])\n",
    "    tokens = re.split('\\W+', tweet)\n",
    "    tweet = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    return tweet\n",
    "\n",
    "# Change any word belonging to the same word-family into a common word (changing/changes/changed.. ==> change)\n",
    "def lemmatization(token_tweet):\n",
    "    tweet = [wn.lemmatize(word) for word in token_tweet]\n",
    "    return tweet\n",
    "\n",
    "# Concatenate the tokennized tweet into a all text like at the beginning\n",
    "def concatenate(lst):\n",
    "    concatenate_tweet = ''\n",
    "    for elem in lst:\n",
    "        concatenate_tweet = concatenate_tweet + ' ' + elem\n",
    "    return concatenate_tweet\n",
    "\n",
    "def remove_digit(txt):\n",
    "    txt = ''.join([i for i in txt if not i.isdigit()])\n",
    "    return txt\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    print(\"Inside clean_data\")\n",
    "    df['text'] = df['text'].apply(lambda x : clean_tweet(x))\n",
    "    print(\"Clean_tweet DONE\")\n",
    "    df['text'] = df['text'].apply(lambda x : lemmatization(x))\n",
    "    print(\"Lemmatization DONE\")\n",
    "    df['text'] = df['text'].apply(lambda x : concatenate(x))\n",
    "    print(\"Concatenate DONE\")\n",
    "    # df['text'] = df['text'].apply(lambda x : clean_tweet(x))\n",
    "    # print(\"Second clean tweet DONE\")\n",
    "    # df['text'] = df['text'].apply(lambda x : concatenate(x))\n",
    "    # print(\"Second concatenate DONE\")\n",
    "    # df['text'] = df['text'].apply(lambda x : remove_digit(x))\n",
    "    # print(\"Remove digit DONE\")\n",
    "    return df\n",
    "\n",
    "# export the dataframe to a csv file\n",
    "def export_to_csv(df, filename):\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside clean_data\n",
      "Clean_tweet DONE\n",
      "Lemmatization DONE\n",
      "Concatenate DONE\n",
      "Inside clean_data\n",
      "Clean_tweet DONE\n",
      "Lemmatization DONE\n",
      "Concatenate DONE\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"../data/\"\n",
    "\n",
    "POSITIVE_DATASET = DATA_FOLDER+\"train_pos.txt\"\n",
    "NEGATIVE_DATASET = DATA_FOLDER+\"train_neg.txt\"\n",
    "\n",
    "TEST_DATASET = DATA_FOLDER+\"test_data.txt\"\n",
    "\n",
    "pos_data = pd.read_fwf(POSITIVE_DATASET, header=None, names=[\"text\"]).drop_duplicates()\n",
    "pos_data[\"labels\"] = 1\n",
    "neg_data = pd.read_fwf(NEGATIVE_DATASET, header=None, names=[\"text\"]).drop_duplicates()\n",
    "neg_data[\"labels\"] = 0\n",
    "\n",
    "\n",
    "df_test = loadtxt(TEST_DATASET, delimiter=\",\", dtype=str, usecols=1)\n",
    "test_data = pd.DataFrame(df_test, columns=['text']).drop_duplicates()\n",
    "\n",
    "# train_data = pd.concat([pos_data, neg_data], ignore_index=True)\n",
    "\n",
    "pos_data_cleaned = clean_data(pos_data)\n",
    "neg_data_cleaned = clean_data(neg_data)\n",
    "\n",
    "test_data_cleaned = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv\n",
    "\n",
    "pos_data_cleaned.to_csv('../data/pos_data_cleaned.csv')\n",
    "neg_data_cleaned.to_csv('../data/neg_data_cleaned.csv')\n",
    "\n",
    "test_data_cleaned.to_csv('../data/test_data_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
