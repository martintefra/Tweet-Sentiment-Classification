{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkdTZPs-X10w",
        "outputId": "1f70374a-f913-440b-d514-4248bbc84568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blZBSwYTxxw8",
        "outputId": "5ba8b387-07db-4933-9060-13066ad917d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkUbJseFxIVY",
        "outputId": "cfccc543-311b-4702-bb97-ef9f0347b463"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1PBpF-mxZK5",
        "outputId": "4af185fe-967a-44ea-b949-ca0a39ad656a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTaSXnzEJTPU"
      },
      "outputs": [],
      "source": [
        "#pip install compound-word-splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1sedgl7V0Sm",
        "outputId": "5ecf19c2-e0c0-49cc-cbb4-4753913129bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wordninja\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[K     |████████████████████████████████| 541 kB 7.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541552 sha256=29258b9fc860456fecbb98d66b41bc72a53fde43c223a6ac16960c249347b70d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/08/fb/98831d1c2702c8352a339f91104515eab5d7906d4118eeeaed\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install wordninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQGqF6RzuiPx",
        "outputId": "c211571a-e083-467d-94e8-34c3542c880e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 63.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdt8xkCrJNNe",
        "outputId": "0477d27b-c45c-431c-ada2-5b546ab4d838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,312 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!apt install -qq enchant\n",
        "!pip install pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYM2Ld_8e3O",
        "outputId": "5e033265-6014-448a-fc4d-1875d07a829a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 22 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BgUumJ1aPMKC"
      },
      "outputs": [],
      "source": [
        "#Basic Libraries\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "\n",
        "#Sklearn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "#xgboost\n",
        "from xgboost import XGBClassifier\n",
        "#Build the LSTM model\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle as cPickle\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import  Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "#for expansion the contractions for instance I'll or I've been to I will and I have been and not only \n",
        "import contractions \n",
        "\n",
        "#import emoticons in order to replace them with appropriate words\n",
        "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n",
        "\n",
        "\n",
        "#read html files \n",
        "import requests\n",
        "\n",
        "#split the \n",
        "import wordninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0z4uPk-We-S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vSK5ojfZuskX"
      },
      "outputs": [],
      "source": [
        "\n",
        "#we use these two dataset in ADA homeworks\n",
        "url_positive = \"https://ptrckprry.com/course/ssd/data/positive-words.txt\"\n",
        "rsp = requests.get(url_positive)\n",
        "lines = rsp.text.strip(\"\\n\").split(\"\\n\")\n",
        "positive_words = lines[lines.index('a+'):]\n",
        "\n",
        "url_negative = \"https://ptrckprry.com/course/ssd/data/negative-words.txt\"\n",
        "rsp = requests.get(url_negative)\n",
        "lines = rsp.text.strip(\"\\n\").split(\"\\n\")\n",
        "negative_words = lines[lines.index('2-faced'):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hZpQPYuYuKcs"
      },
      "outputs": [],
      "source": [
        "def expansion_patterns(text):\n",
        "\n",
        "    expansion_patterns = [(' nd ',' and '),(' wa ',' was '),(' donnow ',' do not know '),(' i\\'ts ','it is '),\n",
        "                      (' dem ',' them '),(' #+ha+ha ',' haha '),(' i\\'ts ','it is '),(' i\\'ts ','it is '),(' n+a+h+ ', ' no '),\n",
        "                      (' n+a+ ', ' no '),(' w+o+w+', 'wow '),('y+a+y+', 'yay'),('y+[e,a]+s+', 'yes'),\n",
        "                      (' ya ', ' you '),('n+o+', 'no'),('a+h+','ah'),('muah','kiss'),(' y+u+p+ ', ' yes '),(' y+e+p+ ', ' yes '),\n",
        "                      (' ima ', ' i am going to '),(' woah ', ' wow '),(' wo ', ' wow '),(' aw ', ' cute '), \n",
        "                      (' lmao ', ' haha '),(' lol ', ' haha ')]\n",
        "    patterns = [(re.compile(regex_exp, re.IGNORECASE), replacement) for (regex_exp, replacement) in expansion_patterns]\n",
        "    for (pattern, replacement) in patterns:\n",
        "        (text, _) = re.subn(pattern, replacement, text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yEuuxE6IunIk"
      },
      "outputs": [],
      "source": [
        "\n",
        "#A more robust preprocessing phase \n",
        "#import the stopword list from the spacy library \n",
        "#Counter appliation in order to improve the running \n",
        "EMOTICONS_EMO[':d'] = 'laughing '\n",
        "EMOTICONS_EMO['<3'] = 'red heart'\n",
        "\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "spacy_stopwords = sp.Defaults.stop_words\n",
        "stopwords_dict = Counter(spacy_stopwords)\n",
        "lemmatizer =WordNetLemmatizer()\n",
        "\n",
        "COUNT = 0\n",
        "\n",
        "def increment():\n",
        "    global COUNT\n",
        "    COUNT = COUNT+1\n",
        "\n",
        "\n",
        "#cleaning \"pipeline\"  \n",
        "def clean_data(text, stopwords, lemmatization):\n",
        "\n",
        "      #TODO\n",
        "      #map the emoji to lexicon \n",
        "      #for instance if we have 'text text <3 :d text :D' will be   'text text red heart positive laughing positive text Laughing'\n",
        "      text = ' '.join(EMOTICONS_EMO.get(word) if word in EMOTICONS_EMO.keys() else word for word in text.split() ) \n",
        "      #print('emoji done')\n",
        "\n",
        "      #perform casefolding\n",
        "      text =text.casefold()\n",
        "      #print('casefold done')\n",
        "\n",
        "      #remove punctuations for each twitter\n",
        "      text = ' '.join(text_ for text_ in text.split() if text_ not in string.punctuation)\n",
        "      #print('punctuations done')\n",
        "\n",
        "      #remove all numbers not just digits since doesn't give so much information for the purpose of sentimental analysis\n",
        "      #for instance '#5words 625' with be '#words and then will be \"words\" after removing the hashtags in the later phase of preprocessing\n",
        "      \n",
        "      text = ' '.join(re.sub('(\\d+(\\.\\d+)?)','',word) if re.search('(\\d+(\\.\\d+)?)',word) else word for word in text.split() ).strip()\n",
        "      #print('remove numbers done')\n",
        "\n",
        "      #remove different tags for instance \"<user>,<url>\" for each twitter\n",
        "      text = re.sub('<[^<]+?>','', text)\n",
        "      #print('remove different tags done')\n",
        "\n",
        "      #remove multiply commas and dots everywhere in tweets      \n",
        "      text = re.sub('\\.|,*','', text)\n",
        "      #print('remove multiply commas done')\n",
        "\n",
        "      #expansion patterns\n",
        "      text=expansion_patterns(text)\n",
        "      #print('remove expansion patterns')\n",
        "\n",
        "      #contractions from the library \"coz\"(because) is too powerful for instance coz to because or I'll to I will or don't to do not or even dont to do not \n",
        "      text = ' '.join(contractions.fix(text_) for text_ in text.split() ) \n",
        "      #print('contractions done')\n",
        "\n",
        "      if stopwords:\n",
        "          #remove the stopwords\n",
        "          text = ' '.join([word for word in text.split() if word not in stopwords_dict])         \n",
        "      #print('stopwords done')\n",
        "      \n",
        "      #split th words within a hashtags , if it's unsplittable we will remove the word i.e if #happythoughts with be happy thoughts using the library compound word splitter \n",
        "      #for instance the tweet \n",
        "      #<user> hahahhahaha aw dont cry #thinkhappythoughts .. yeah right #cryyoureffingeyesout will be\n",
        "      #<user> hahahhahaha aw dont cry think happy thoughts .. yeah right cry your effing eyes out\n",
        "      \n",
        "      text=' '.join( ' '.join(wordninja.split(word_[1:])) if word_.startswith('#') else word_ for word_ in text.split() )\n",
        "      #print('split hashtag done')\n",
        "\n",
        "      if lemmatization :\n",
        "          #perform lemmatization\n",
        "          text = ' '.join(lemmatizer.lemmatize(text_)  for text_ in text.split() )\n",
        "      #print('lemmatization done')\n",
        "\n",
        "      # use the positive and negative sentimental analysis adding the appropriate words in each tweet using predefined dictionaries/vocabularies\n",
        "      #https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets we used the list of postive and negative lexicon word to add postive and negative token to our tweets\n",
        "      \n",
        "      text = ' '.join(word+\" positive\" if word in positive_words else word for word in text.split() )\n",
        "      text = ' '.join(word+\" negative\" if word in negative_words else word for word in text.split() )\n",
        "      #print('postive done')\n",
        "\n",
        "      #remove the tokens length less than 2 again if some may appear after the above preprocessing\n",
        "      text = ' '.join(text_ for text_ in text.split() if len(text_)>2)\n",
        "      #print('#remove the tokens done')\n",
        "\n",
        "\n",
        "      increment()\n",
        "      if(COUNT%10000==0):\n",
        "        print(COUNT)\n",
        "      return  text.strip()\n",
        "\n",
        "\n",
        "#Load the data and run the preprocessor pipeline \n",
        "class Preprocessor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Init function\n",
        "        \"\"\"\n",
        "    def load_data(preprocessed=True,Train_data=True):\n",
        "        DIRECTORY1 = \"/content/drive/MyDrive/train_pos_full.txt\"\n",
        "        DIRECTORY2 = \"/content/drive/MyDrive/train_neg_full.txt\"\n",
        "\n",
        "        \n",
        "        #import the data\n",
        "\n",
        "        if Train_data==True:\n",
        "          pos_data = pd.read_fwf(DIRECTORY1, header=None, names=[\"tweets\"])\n",
        "          pos_data[\"label\"] = 1.0\n",
        "\n",
        "          neg_data = pd.read_fwf(DIRECTORY2, header=None, names=[\"tweets\"])\n",
        "          neg_data[\"label\"] = 0.0\n",
        "          data = pd.concat([pos_data, neg_data], ignore_index=True)\n",
        "          np.random.seed(500)\n",
        "          #shuffle the merge data\n",
        "          data = data.iloc[np.random.permutation(len(data))]\n",
        "\n",
        "          #print(pos_data.isnull().any(axis=1))\n",
        "        else:\n",
        "          with open('/content/drive/MyDrive/test_data.txt') as f:\n",
        "            data = f.readlines()\n",
        "          data = pd.DataFrame(data,columns=['tweets'])\n",
        "          data.tweets=data.tweets.apply(lambda x :x[x.find(',')+1:])\n",
        "        #data.dropna(subset = [\"tweets\"], inplace=True)\n",
        "        data['tweets']=data['tweets'].apply(lambda x : clean_data(x, stopwords=True,lemmatization=True))\n",
        "        \n",
        "        #remove empty lines if any  \n",
        "        #data.dropna(subset = [\"tweets\"], inplace=True)\n",
        "\n",
        "        #X = data['tweets'].values\n",
        "        #y = np.stack((data['positive'],data['negative']),axis=-1)\n",
        "\n",
        "        return data#np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-1Xv7RiRzDc",
        "outputId": "fbca09a2-bc00-487d-8e86-dfc2c4581151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_test = Preprocessor.load_data(preprocessed=True,Train_data=False)\n",
        "#it takes ~1 minute to run\n",
        "\n",
        "\n",
        "#powerful preprocessing\n",
        "#,... currently workn out ... <user> park ) #anycompany ? \n",
        "#currently workn park company\n",
        "\n",
        "#OR\n",
        "#3417,<user> loool 7yaaatii mbyn alejtehaad hhh :p p mnn jddd <3 3 , abshrk ana b3d praise is due to allah =) ) :p p\n",
        "#loool yaaatii mbyn alejtehaad hhh mnn jddd red heart positive positive abshrk ana praise positive allah happy positive face smiley"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "haUj7IyoZI9t",
        "outputId": "1e8a7d16-f7b9-433c-abf7-259165fd40dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ec6aaa18-4fc7-469a-92c8-d3e67251e9d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sea doo pro sea scooter sport portable positiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shuck work positive week come cheer positive b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stay away bug negative baby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>madam haha perfectly positive fine positive co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fall negative asleep watching wake headache ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>nice positive time friend lastnite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>stop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>daughter dvd two-time oscar winner positive sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>fun positive class sweetcheeks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>making difference recreational leadership prog...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec6aaa18-4fc7-469a-92c8-d3e67251e9d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec6aaa18-4fc7-469a-92c8-d3e67251e9d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec6aaa18-4fc7-469a-92c8-d3e67251e9d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                 tweets\n",
              "0     sea doo pro sea scooter sport portable positiv...\n",
              "1     shuck work positive week come cheer positive b...\n",
              "2                           stay away bug negative baby\n",
              "3     madam haha perfectly positive fine positive co...\n",
              "4     fall negative asleep watching wake headache ne...\n",
              "...                                                 ...\n",
              "9995                 nice positive time friend lastnite\n",
              "9996                                               stop\n",
              "9997  daughter dvd two-time oscar winner positive sa...\n",
              "9998                     fun positive class sweetcheeks\n",
              "9999  making difference recreational leadership prog...\n",
              "\n",
              "[10000 rows x 1 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdqw0s0lR0PX"
      },
      "outputs": [],
      "source": [
        "X_test.tweets[3416]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMkrVCGy5KYt"
      },
      "outputs": [],
      "source": [
        "#X_full = Preprocessor.load_data(preprocessed=True,Train_data=True)\n",
        "#it takes ~2 hour to complete the preprocessing phase in the entire 2.5M tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0eEdVvAlfvaW"
      },
      "outputs": [],
      "source": [
        "X_positive = pd.read_csv('/content/drive/MyDrive/X_positive_preprocessed.csv')\n",
        "X_negative = pd.read_csv('/content/drive/MyDrive/X_negative_preprocessed.csv')\n",
        "X_full_preprocessed = pd.concat([X_positive, X_negative], ignore_index=True)\n",
        "np.random.seed(500)\n",
        "#shuffle the merge data\n",
        "X_full_preprocessed = X_full_preprocessed.iloc[np.random.permutation(len(X_full_preprocessed))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Mcn5s8SC5Zw7"
      },
      "outputs": [],
      "source": [
        "X_full_preprocessed.tweets=X_full_preprocessed.tweets.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3oDZjx8FhhM-",
        "outputId": "b4617187-19eb-4edb-e172-2b4235a34f39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b34fc526-6fe0-4f87-8c23-bca78b8db50b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>409175</th>\n",
              "      <td>follow xxx</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2459601</th>\n",
              "      <td>rita party look sweet positive dreaming</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231087</th>\n",
              "      <td>zinc plated steel spring pin diameter length p...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2363496</th>\n",
              "      <td>sweet positive home farm maple pecan granola o...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854193</th>\n",
              "      <td>hahaha love positive calling funday yes come s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1854397</th>\n",
              "      <td>getting closer god paperback book contends bet...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958737</th>\n",
              "      <td>follow follow</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>831297</th>\n",
              "      <td>egypt</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2245559</th>\n",
              "      <td>fuck negative shit negative</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375066</th>\n",
              "      <td>coming live</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b34fc526-6fe0-4f87-8c23-bca78b8db50b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b34fc526-6fe0-4f87-8c23-bca78b8db50b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b34fc526-6fe0-4f87-8c23-bca78b8db50b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    tweets  label\n",
              "409175                                          follow xxx    1.0\n",
              "2459601            rita party look sweet positive dreaming    0.0\n",
              "2231087  zinc plated steel spring pin diameter length p...    0.0\n",
              "2363496  sweet positive home farm maple pecan granola o...    0.0\n",
              "854193   hahaha love positive calling funday yes come s...    1.0\n",
              "...                                                    ...    ...\n",
              "1854397  getting closer god paperback book contends bet...    0.0\n",
              "958737                                       follow follow    1.0\n",
              "831297                                               egypt    1.0\n",
              "2245559                        fuck negative shit negative    0.0\n",
              "1375066                                        coming live    0.0\n",
              "\n",
              "[2500000 rows x 2 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_full_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ptDRhrydurrz"
      },
      "outputs": [],
      "source": [
        "y_train=X_full_preprocessed.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K72koNKUuzFs"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters=\"\")\n",
        "tokenizer.fit_on_texts(X_full_preprocessed.tweets)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_full_preprocessed.tweets)#convert each word to a integer based on the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "724ZylTD5nrr"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(tokenizer.word_index)+1\n",
        "max_len=40\n",
        "X_train = pad_sequences(X_train, padding='post'  ,maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zURKpFWzxR2k"
      },
      "outputs": [],
      "source": [
        "X_test = tokenizer.texts_to_sequences(X_test.tweets)#covert each word to a integer based on the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_xldwGy8xMBg"
      },
      "outputs": [],
      "source": [
        "X_test = pad_sequences(X_test, padding='post'  ,maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_XolwCXyfkD",
        "outputId": "66ac05f9-11d2-4c79-dcde-8dc6f13084d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  10,   66,    0, ...,    0,    0,    0],\n",
              "       [6683,  244,   47, ...,    0,    0,    0],\n",
              "       [1525, 2597,  465, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [4274,    0,    0, ...,    0,    0,    0],\n",
              "       [ 121,    2,   98, ...,    0,    0,    0],\n",
              "       [ 150,   79,    0, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivcb5RdAyZx1",
        "outputId": "e7f6de5d-341e-4128-b0cd-c7f8a11f860c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1262,  3573,   460, ...,     0,     0,     0],\n",
              "       [ 7750,    30,     1, ...,     0,     0,     0],\n",
              "       [  193,   143,  1865, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 1199,   127, 25666, ...,     0,     0,     0],\n",
              "       [   90,     1,   151, ...,     0,     0,     0],\n",
              "       [  224,  1905, 17051, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w4sRJEU5JxD"
      },
      "source": [
        "**USE THE PRETRAINED GLVOE EMBEDDINGS FOR OUR TEXT TO VECTOR REPRESENTATIONS SINCE IS MORE POWERFUL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "o5YhC7jTuGhK"
      },
      "outputs": [],
      "source": [
        "\n",
        "#retrieve the pretrained embeddings and store them as a dictionary\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsJmQtae0uxo"
      },
      "outputs": [],
      "source": [
        "embeddings_index.get(':')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh-Z5pMIwK-Y",
        "outputId": "85bb1fba-8390-4b16-9fda-00a021b07a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(393162, 200)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#form our embedding matrix for each word that appears in our dataset based on pretrained glove embeddings\n",
        "embedding_matrix = np.zeros((vocab_size , 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(embedding_matrix.shape)\n",
        "cPickle.dump([embedding_matrix],open('/content/drive/MyDrive/embedding_matrix_full_preprocessing.dat', 'wb'))#write the embedding_matrix in file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J22Zlr8c7A_",
        "outputId": "c424f3e1-87c4-4262-d4be-664b1ef458b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.66254997,  0.44600999, -0.31863001, ...,  0.16424   ,\n",
              "         0.078667  , -0.78938001],\n",
              "       [ 0.82006001,  0.26276001, -0.42973   , ...,  0.42840999,\n",
              "        -0.28101999, -0.54276001],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0hBFLzYNYC"
      },
      "source": [
        "**TRAIN A GLOVE MODEL FOR THE EMBEDDINGS ON MY CORPUS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GKsAFzld3gYA",
        "outputId": "6fbf742e-d5f1-4568-faf9-410c0a434a37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# instantiate the corpus\\ncorpus = Corpus() \\n# this will create the word co occurence matrix \\ncorpus.fit(X_200.tweets, window=1000)\\n\\n# instantiate the model\\nglove_model = Glove(no_components=200, learning_rate=0.1)\\n\\n# and fit over the corpus matrix\\nglove_model.fit(corpus.matrix, epochs=20, no_threads=32)\\n\\n# finally we add the vocabulary to the model\\nglove_model.add_dictionary(corpus.dictionary)\\n'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# instantiate the corpus\n",
        "corpus = Corpus() \n",
        "# this will create the word co occurence matrix \n",
        "corpus.fit(X_200.tweets, window=1000)\n",
        "\n",
        "# instantiate the model\n",
        "glove_model = Glove(no_components=200, learning_rate=0.1)\n",
        "\n",
        "# and fit over the corpus matrix\n",
        "glove_model.fit(corpus.matrix, epochs=20, no_threads=32)\n",
        "\n",
        "# finally we add the vocabulary to the model\n",
        "glove_model.add_dictionary(corpus.dictionary)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq-mqyWTAgHW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#form our embedding matrix for each word that appears in our dataset based on trained glove embeddings on our corpus\n",
        "embedding_matrix = np.zeros((max_features + 1, 200))\n",
        "for word, i in word_index.items():\n",
        "    if i > max_features:\n",
        "        continue\n",
        "    embedding_vector = glove_model.word_vectors[glove_model.dictionary[word]]\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "           \n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCyQ_YWiY7mt"
      },
      "source": [
        "**DIFFERENT NEURAL NETWORK MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rpuevmvjOnLL"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.layers import Embedding, SpatialDropout1D , Conv1D, MaxPooling1D\n",
        "from keras.layers import Bidirectional, LSTM, Dense, Dropout,Masking,Activation\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam  # SGD, RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk_Qa8ujZFqB",
        "outputId": "6efefd92-ea13-4ae7-fb70-be4b913e7ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 200)           78632400  \n",
            "                                                                 \n",
            " masking (Masking)           (None, 40, 200)           0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 1024)             2920448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81,618,513\n",
            "Trainable params: 2,986,113\n",
            "Non-trainable params: 78,632,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#MODEL 1 \n",
        "#vocab_length=max_features+1\n",
        "embedding_size=200\n",
        "num_of_words=40 # train_sequences.shape[1]\n",
        "\n",
        "#first model : simple neural network\n",
        "model1 = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False,mask_zero=True) #trainable set to False bc we use the downloaded dict\n",
        "model1.add(embedding_layer)\n",
        "model1.add(Masking(mask_value=0.0)) #need masking layer to not train on padding (so for that words whicha weren't in the pretrained glove embeddings so their representation is full of zeros)\n",
        "model1.add(Bidirectional(LSTM(512)))\n",
        "model1.add(Dense(64, activation='relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(1))\n",
        "model1.add(Activation('sigmoid'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model1.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxv_EfbsDnYb"
      },
      "outputs": [],
      "source": [
        "#tf.keras.utils.plot_model(model1, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7hTuE8tgx_2",
        "outputId": "322ec112-7bdf-41a0-ee2a-883a5a5850a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "2198/2198 [==============================] - 671s 305ms/step - loss: 0.4126 - acc: 0.8048 - val_loss: 0.3932 - val_acc: 0.8139\n",
            "Epoch 2/6\n",
            "2198/2198 [==============================] - 649s 295ms/step - loss: 0.3868 - acc: 0.8191 - val_loss: 0.3817 - val_acc: 0.8202\n",
            "Epoch 3/6\n",
            "2198/2198 [==============================] - 644s 293ms/step - loss: 0.3665 - acc: 0.8296 - val_loss: 0.3780 - val_acc: 0.8233\n",
            "Epoch 4/6\n",
            "2198/2198 [==============================] - 651s 296ms/step - loss: 0.3424 - acc: 0.8417 - val_loss: 0.3813 - val_acc: 0.8237\n",
            "Epoch 5/6\n",
            "2198/2198 [==============================] - 655s 298ms/step - loss: 0.3131 - acc: 0.8561 - val_loss: 0.3988 - val_acc: 0.8217\n",
            "Epoch 6/6\n",
            "2198/2198 [==============================] - 642s 292ms/step - loss: 0.2806 - acc: 0.8719 - val_loss: 0.4329 - val_acc: 0.8181\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc42b3c6490>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS=6\n",
        "BATCH_SIZE=1024\n",
        "\n",
        "\n",
        "model1.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
        "#Evaluate model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "n60DU9KOy3b3"
      },
      "outputs": [],
      "source": [
        "model1.save('/content/drive/MyDrive/model_1',save_format=\"h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic2QF7XPa7RE"
      },
      "source": [
        "**MODEL 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbFk6-Ok-ITF",
        "outputId": "0bacb388-7aab-43ea-da52-076475a18b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 40, 200)           78632400  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               120400    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                6464      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 78,759,329\n",
            "Trainable params: 126,929\n",
            "Non-trainable params: 78,632,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#MODEL 2 # has potential for more accurate predictions\n",
        "#vocab_length=max_features+1\n",
        "embedding_size=200 \n",
        "num_of_words=40 # train_sequences.shape[1]\n",
        "\n",
        "#first model : simple neural network\n",
        "model2 = Sequential()\n",
        "embedding_layer2 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False) #trainable set to False bc we use the downloaded dict\n",
        "model2.add(embedding_layer2)\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(64))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model2.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "c8g9xXNp-OId"
      },
      "outputs": [],
      "source": [
        "#tf.keras.utils.plot_model(model2, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEgSyAmv_umx",
        "outputId": "2368842f-cfb6-488f-e28c-4505edb2f800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "17579/17579 [==============================] - 103s 6ms/step - loss: 0.4171 - acc: 0.8005 - val_loss: 0.3972 - val_acc: 0.8115\n",
            "Epoch 2/6\n",
            " 5362/17579 [========>.....................] - ETA: 1:04 - loss: 0.3940 - acc: 0.8139"
          ]
        }
      ],
      "source": [
        "EPOCHS=6\n",
        "BATCH_SIZE=128\n",
        "\n",
        "\n",
        "model2.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
        "#Evaluate model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6AHlH3Uy7TW"
      },
      "outputs": [],
      "source": [
        "model2.save('/content/drive/MyDrive/model_2',save_format=\"h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoNIxQ08bDyk"
      },
      "source": [
        "**MODEL 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24S2Y0zBGvsS"
      },
      "outputs": [],
      "source": [
        "#MODEL 3\n",
        "#vocab_length=max_features+1\n",
        "embedding_size=200\n",
        "num_of_words=40 # train_sequences.shape[1]\n",
        "\n",
        "#first model : simple neural network\n",
        "model3 = Sequential()\n",
        "embedding_layer3 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False) #trainable set to False bc we use the downloaded dict\n",
        "model3.add(embedding_layer3)\n",
        "model3.add(LSTM(1024))\n",
        "model3.add(Dropout(0.4))\n",
        "model3.add(Dense(512, activation='relu'))\n",
        "model3.add(Dropout(0.4))\n",
        "model3.add(Dense(512,activation='relu'))\n",
        "model3.add(Dropout(0.4))\n",
        "model3.add(Dense(512,activation='relu'))\n",
        "\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model3.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3hR0NAdeeKE"
      },
      "outputs": [],
      "source": [
        "#tf.keras.utils.plot_model(model3, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhmaSFQyHuvy"
      },
      "outputs": [],
      "source": [
        "EPOCHS=6\n",
        "BATCH_SIZE=128\n",
        "\n",
        "\n",
        "model3.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n",
        "#Evaluate model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j6j0SHIzAW8"
      },
      "outputs": [],
      "source": [
        "model3.save('/content/drive/MyDrive/model_3',save_format=\"h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwSiNcsFbLOI"
      },
      "source": [
        "**MODEL 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spCudgLEatPV"
      },
      "outputs": [],
      "source": [
        "#MODEL 4\n",
        "\n",
        "#vocab_length=max_features+1\n",
        "embedding_size=200\n",
        "num_of_words=40 # train_sequences.shape[1]\n",
        "\n",
        "#first model : simple neural network\n",
        "model4 = Sequential()\n",
        "embedding_layer2 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False,mask_zero=True) #trainable set to False bc we use the downloaded dict\n",
        "model4.add(embedding_layer2)\n",
        "model4.add(Masking(mask_value=0.0)) #need masking layer to not train on padding (so for that words whicha weren't in the pretrained glove embeddings so their representation is full of zeros)\n",
        "model4.add(LSTM(512,return_sequences=True))\n",
        "model4.add(Dropout(0.3))\n",
        "model4.add(LSTM(512,return_sequences=True))\n",
        "model4.add(LSTM(265))\n",
        "model4.add(Dense(64, activation='relu'))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(1))\n",
        "model4.add(Activation('sigmoid'))\n",
        "\n",
        "model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model4.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCyEiWwYayhG"
      },
      "outputs": [],
      "source": [
        "#tf.keras.utils.plot_model(model4, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcPUABRta1vi"
      },
      "outputs": [],
      "source": [
        "EPOCHS=6\n",
        "BATCH_SIZE=1024\n",
        "\n",
        "\n",
        "model4.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQl1XCOTzHKA"
      },
      "outputs": [],
      "source": [
        "model4.save('/content/drive/MyDrive/model_4',save_format=\"h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpS1svSUcyER"
      },
      "source": [
        "**MODEL 5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuxJKt-NdIAE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDd33TE7dIJn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#MODEL 5\n",
        "#vocab_length=max_features+1\n",
        "#embedding_size=200\n",
        "num_of_words=40 # train_sequences.shape[1]\n",
        "embedding_size=200\n",
        "#first model : simple neural network\n",
        "model5 = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=num_of_words , trainable=False,mask_zero=True) #trainable set to False bc we use the downloaded dict\n",
        "model5.add(embedding_layer)\n",
        "model5.add(Masking(mask_value=0.0)) #need masking layer to not train on padding (so for that words whicha weren't in the pretrained glove embeddings so their representation is full of zeros)\n",
        "model5.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model5.add(MaxPooling1D(pool_size=2))\n",
        "model5.add(LSTM(256))\n",
        "model5.add(Dense(64, activation='relu'))\n",
        "model5.add(Dropout(0.5))\n",
        "model5.add(Dense(1))\n",
        "model5.add(Activation('sigmoid'))\n",
        "\n",
        "model5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model5.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDUkKOcldVSA"
      },
      "outputs": [],
      "source": [
        "#tf.keras.utils.plot_model(model5, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHk2M7y1ddc7"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPOCHS=6\n",
        "BATCH_SIZE=1024\n",
        "model5.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipn6rsfuzKcb"
      },
      "outputs": [],
      "source": [
        "model5.save('/content/drive/MyDrive/model_5',save_format=\"h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnLVSaLcdmvM"
      },
      "source": [
        "**MODEL 6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Mox9ncdorm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjV1xEAEd0Wj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#MODEL 1 \n",
        "\n",
        "#vocab_length=max_features+1\n",
        "embedding_size=200\n",
        "num_of_words=40 # train_sequences.shape[1]\n",
        "\n",
        "#first model : simple neural network\n",
        "model6 = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_len , trainable=False) #trainable set to False bc we use the downloaded dict\n",
        "model6.add(embedding_layer)\n",
        "model6.add(Bidirectional(LSTM(1024)))\n",
        "model6.add(Dense(512, activation='relu'))\n",
        "model6.add(Dropout(0.4))\n",
        "model6.add(Dense(512, activation='relu'))\n",
        "model6.add(Dropout(0.4))\n",
        "model6.add(Dense(512, activation='relu'))\n",
        "model6.add(Dropout(0.4))\n",
        "model6.add(Dense(1))\n",
        "model6.add(Activation('sigmoid'))\n",
        "\n",
        "model6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model6.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1Igukhu53Aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD0_gxZmd5iA"
      },
      "outputs": [],
      "source": [
        "#tf.keras.utils.plot_model(model6, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQdazrg7d9wh"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPOCHS=6\n",
        "BATCH_SIZE=1024\n",
        "\n",
        "\n",
        "model6.fit(X_train,y_train , batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split = 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLyTdE2ZzNqS"
      },
      "outputs": [],
      "source": [
        "model6.save('/content/drive/MyDrive/model_6',save_format=\"h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBCy5xNRAGwK"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model1=keras.models.load_model('/content/drive/MyDrive/model_1')\n",
        "model2=keras.models.load_model('/content/drive/MyDrive/model_2')\n",
        "model3=keras.models.load_model('/content/drive/MyDrive/model_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ER21UYgMxk"
      },
      "outputs": [],
      "source": [
        "train1 = model1.predict(X_train, batch_size=128)\n",
        "test1 = model1.predict(X_test)\n",
        "\n",
        "train2 = model2.predict(X_train, batch_size=128)\n",
        "test2 = model2.predict(X_test)\n",
        "\n",
        "train3 = model3.predict(X_train, batch_size=128)\n",
        "test3 = model3.predict(X_test)\n",
        "\n",
        "train4 = model4.predict(X_train, batch_size=128)\n",
        "test4 = model4.predict(X_test)\n",
        "\n",
        "train5 = model5.predict(X_train, batch_size=128)\n",
        "test5 = model5.predict(X_test)\n",
        "\n",
        "#train6 = model6.predict(X_train, batch_size=128)\n",
        "#test6 = model6.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwjfmEKyiOfM"
      },
      "outputs": [],
      "source": [
        "train = np.hstack((train1, train2, train3, train4, train5))\n",
        "test = np.hstack((test1, test2, test3, test4, test5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZUMa1WhiwxB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import xgboost as xgb\n",
        "model = xgb.XGBClassifier().fit(train, y_train)\n",
        "y_pred = model.predict(test)\n",
        "y_pred=[-1 if y_p<0.5 else 1 for y_p in y_pred ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHSSs74yxyi5"
      },
      "outputs": [],
      "source": [
        "index=[]\n",
        "for i in range(1,10001):\n",
        "  index.append(i)\n",
        "index=pd.DataFrame(index,columns=['Id'])\n",
        "predictions=pd.DataFrame(y_pred,columns=[\"Prediction\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K46L0bLmx3-Y"
      },
      "outputs": [],
      "source": [
        "predictions_final=pd.concat([index, predictions], ignore_index=False,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqtCuE7sx7Kp"
      },
      "outputs": [],
      "source": [
        "predictions_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXfP0zOEx6Mf"
      },
      "outputs": [],
      "source": [
        "predictions_final.to_csv('/content/drive/MyDrive/submission2.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Aug  5 2022, 15:21:02) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
