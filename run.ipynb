{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352de6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "#from implementations import *\n",
    "#from split_data import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bd0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d00539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Main external library : Natural Language Toolkit (nltk)\n",
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8ff2d",
   "metadata": {},
   "source": [
    "### Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b670236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"data/\"\n",
    "\n",
    "# Note: it seems that the data is already to lower case, so no need to apply lower() to the text\n",
    "pos_data = pd.read_fwf(\"data/train_pos.txt\", header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
    "pos_data[\"labels\"] = 1\n",
    "neg_data = pd.read_fwf('data/train_neg.txt', header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
    "neg_data[\"labels\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0991a11",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e4eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/teframartin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50dae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word that is so common that there is no need to use it in a search\n",
    "ENGLISH_STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Adding few extra stop word\n",
    "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS + ['im', 'dont','dunno', 'cant', ' 2 ', \"'s\", ' u ', ' x ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdd884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the two training sets of positive and negative tweets\n",
    "train_data = pd.concat([pos_data, neg_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e64fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common words used in the set of all tweets\n",
    "def get_most_common_words(txt,limit):\n",
    "    return Counter(txt.split()).most_common()[:limit]\n",
    "\n",
    "# Remove from tweets the punctuation and stop words (= a word that is so common that there is no need to use it in a search.)\n",
    "def clean_tweet(tweet):\n",
    "    tweet = \"\".join([w for w in tweet if w not in string.punctuation])\n",
    "    tokens = re.split('\\W+', tweet)\n",
    "    tweet = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    return tweet\n",
    "\n",
    "# Change any word belonging to the same word-family into a common word (changing/changes/changed.. ==> change)\n",
    "def lemmatization(token_tweet):\n",
    "    tweet = [wn.lemmatize(word) for word in token_tweet]\n",
    "    return tweet\n",
    "\n",
    "# Concatenate the tokennized tweet into a all text like at the beginning\n",
    "def concatenate(lst):\n",
    "    concatenate_tweet = ''\n",
    "    for elem in lst:\n",
    "        concatenate_tweet = concatenate_tweet + ' ' + elem\n",
    "    return concatenate_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_tweet transformation\n",
    "train_data['text'] = train_data['text'].apply(lambda x : clean_tweet(x)).apply(lambda x : lemmatization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9de3c8",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2c9a4",
   "metadata": {},
   "source": [
    "1. Initialize a task-specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48f51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel(\"roberta\", \"roberta-base\", use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27a63d",
   "metadata": {},
   "source": [
    "2. Train the model with train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1bdd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3401daf0594de18677207a71bdbfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca91b68419e46ff88057e01f3e92610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53705f0fc7704b33aeb3201e004995a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(114, 0.6658787920809629)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train_data.sample(frac=0.005), output_dir=\"outputs/roberta\", args={\"overwrite_output_dir\": True, \"num_train_epochs\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903877d",
   "metadata": {},
   "source": [
    "3. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58cbfa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa963355589e4093945927c7d16f663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf68b4250934afeb4f41166a60c70b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample an other part of the train full dataset and measure the accuracy\n",
    "# with eval_model()\n",
    "result, model_outputs, wrong_predictions = model.eval_model(train_data.sample(frac=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b5e31b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mcc': 0.2992819375325854,\n",
       "  'tp': 278,\n",
       "  'tn': 311,\n",
       "  'fp': 174,\n",
       "  'fn': 144,\n",
       "  'auroc': 0.7453754824839987,\n",
       "  'auprc': 0.6712635677654164,\n",
       "  'eval_loss': 0.5819035491399598},\n",
       " array([[ 0.069982  , -0.18846767],\n",
       "        [-0.33906487,  0.22555636],\n",
       "        [-0.05236403, -0.10697252],\n",
       "        ...,\n",
       "        [-0.33091792,  0.27910367],\n",
       "        [ 0.16295424, -0.31754607],\n",
       "        [-0.39224237,  0.28036678]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, model_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bb6e7",
   "metadata": {},
   "source": [
    "4. Make predictions on (unlabelled) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c031424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file on the test dataset\n",
    "#  with predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebc07cba07d33741e40e0d779bcc934485fb1a6cfaaf5d05a7b4d79b8578fbe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
