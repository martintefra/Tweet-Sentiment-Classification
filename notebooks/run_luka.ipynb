{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8Fs-Mc91RXol",
      "metadata": {
        "id": "8Fs-Mc91RXol"
      },
      "outputs": [],
      "source": [
        "#Preprocessing\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def my_clean(text, stops, stemming):\n",
        "      text = str(text)\n",
        "      text = re.sub(r\" US \", \" american \", text)\n",
        "      text = text.lower().split()\n",
        "      text = \" \".join(text)\n",
        "      text = re.sub(r\"what's\", \"what is \", text)\n",
        "      text = re.sub(r\"don't\", \"do not \", text)\n",
        "      text = re.sub(r\"aren't\", \"are not \", text)\n",
        "      text = re.sub(r\"isn't\", \"is not \", text)\n",
        "      text = re.sub(r\"%\", \" percent \", text)\n",
        "      text = re.sub(r\"that's\", \"that is \", text)\n",
        "      text = re.sub(r\"doesn't\", \"does not \", text)\n",
        "      text = re.sub(r\"he's\", \"he is \", text)\n",
        "      text = re.sub(r\"she's\", \"she is \", text)\n",
        "      text = re.sub(r\"it's\", \"it is \", text)\n",
        "      text = re.sub(r\"\\'s\", \" \", text)\n",
        "      text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "      text = re.sub(r\"n't\", \" not \", text)\n",
        "      text = re.sub(r\"i'm\", \"i am \", text)\n",
        "      text = re.sub(r\"\\'re\", \" are \", text)\n",
        "      text = re.sub(r\"\\'d\", \" would \", text)\n",
        "      text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "      text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "      text = re.sub(r\"<url\", \" \", text)\n",
        "      text = re.sub(r\",\", \" \", text)\n",
        "      text = re.sub(r\"\\.\", \" \", text)\n",
        "      text = re.sub(r\"!\", \" ! \", text)\n",
        "      text = re.sub(r\"\\/\", \" \", text)\n",
        "      text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "      text = re.sub(r\"\\+\", \" + \", text)\n",
        "      text = re.sub(r\"\\-\", \" - \", text)\n",
        "      text = re.sub(r\"\\=\", \" = \", text)\n",
        "      text = re.sub(r\"'\", \" \", text)\n",
        "      text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "      text = re.sub(r\":\", \" : \", text)\n",
        "      text = re.sub(r\" u s \", \" american \", text)\n",
        "      text = re.sub(r\"\\0s\", \"0\", text)\n",
        "      text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "      text = re.sub(r\"e - mail\", \"email\", text)\n",
        "      text = re.sub(r\"j k\", \"jk\", text)\n",
        "      text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "      text = text.lower().split()\n",
        "      text = [w for w in text if len(w) >= 2]\n",
        "      if stemming and stops:\n",
        "          text = [word for word in text if word not in stopwords.words('english')]\n",
        "          wordnet_lemmatizer = WordNetLemmatizer()\n",
        "          englishStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "          text = [englishStemmer.stem(word) for word in text]\n",
        "          text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "          text = [ word for word in text if word not in stopwords.words('english')]\n",
        "      elif stops:\n",
        "          text = [ word for word in text if word not in stopwords.words('english')]\n",
        "      elif stemming:\n",
        "          wordnet_lemmatizer = WordNetLemmatizer()\n",
        "          englishStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "          text = [englishStemmer.stem(word) for word in text]\n",
        "          text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "      text = \" \".join(text)\n",
        "      return text\n",
        "\n",
        "class Preproccesor:\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Init function\n",
        "        \"\"\"\n",
        "    def load_data(preprocessed=True, stemming_a=True):\n",
        "        DIRECTORY = \"/content/\"\n",
        "\n",
        "        # Note: it seems that the data is already to lower case, so no need to apply lower() to the text\n",
        "        pos_data = pd.read_fwf(DIRECTORY+\"train_neg.txt\", header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
        "        pos_data = pos_data[:1000]\n",
        "        pos_data[\"labels\"] = 1\n",
        "        neg_data = pd.read_fwf(DIRECTORY+'train_pos.txt', header=None, names=[\"text\"]).drop_duplicates().apply(lambda x: x.str.lower())\n",
        "        neg_data = neg_data[:1000]\n",
        "        neg_data[\"labels\"] = 0\n",
        "        data = pd.concat([pos_data, neg_data], ignore_index=True)\n",
        "\n",
        "        np.random.seed(500)\n",
        "        data = data.iloc[np.random.permutation(len(data))]\n",
        "        XT = data['text'].values\n",
        "        X = []\n",
        "        y = data['labels'].values\n",
        "        for x in XT:\n",
        "            if preprocessed:\n",
        "                X.append(my_clean(text=str(x), stops=True, stemming=stemming_a))\n",
        "            else:\n",
        "                X.append(x)\n",
        "        return numpy.array(X), numpy.array(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "PjQ5QUcBRdU4",
      "metadata": {
        "id": "PjQ5QUcBRdU4"
      },
      "outputs": [],
      "source": [
        "#import data\n",
        "# in some cases without stemming and without removing stop words the model can perform better since learn well the meaning of words\n",
        "X, y = Preproccesor.load_data(preprocessed=True, stemming_a=True)\n",
        "\n",
        "class_names = ['Negative', 'Positive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "qVYOWc7ORi8x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVYOWc7ORi8x",
        "outputId": "fc4af518-43e3-459d-d77c-3ed594641017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tweets: 2000\n",
            "Positive tweets: 1000\n",
            "Negative tweets: 1000\n",
            "Sample\n",
            "necklac silver smokeyquartz jewer 18 inch gnklc 322 jewelri ancient time india pow\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(\"Total tweets:\",len(y))\n",
        "print(\"Positive tweets:\",sum(y))\n",
        "print(\"Negative tweets:\",len(y)-sum(y))\n",
        "\n",
        "print(\"Sample\")\n",
        "print(X[3])\n",
        "print(y[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "szjn8BT-X-oN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szjn8BT-X-oN",
        "outputId": "77acc99c-c6c8-4c45-e176-e62178c96333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples:  1400\n",
            "Validation samples:  200\n",
            "Test samples:  400\n"
          ]
        }
      ],
      "source": [
        "#CROSS VALIDATION\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#split train data (20% testing , 10% validation and 70% training)\n",
        "indices = np.arange(len(y))\n",
        "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(list(X), y, indices, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "size = (0.1 * len(y)) / len(train_labels)\n",
        "train_texts, validation_texts, train_labels, validation_labels = train_test_split(list(train_texts), train_labels, stratify=train_labels, test_size=size, random_state=42)\n",
        "\n",
        "print(\"Training samples: \",len(train_labels))\n",
        "print(\"Validation samples: \",len(validation_labels))\n",
        "print(\"Test samples: \",len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "RlUe7zcpVda8",
      "metadata": {
        "id": "RlUe7zcpVda8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from transformers import DistilBertTokenizerFast \n",
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "#Set training arguments\n",
        "training_args = TrainingArguments(\n",
        "    evaluation_strategy='epoch',     # evaluation frequency\n",
        "    save_strategy='epoch',           # model checkpoint frequency\n",
        "    logging_strategy='epoch',        # logging frequency\n",
        "    log_level='warning',             # logging level\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs # checked for different epoches\n",
        "    per_device_train_batch_size=4,   # batch size per device during training\n",
        "    per_device_eval_batch_size=256,   # batch size for evaluation, change according to GPU memory capabilities #checked for different eval batch\n",
        "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.001,              # strength of weight decay\n",
        "    logging_dir='./logs'             # directory for storing logs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "wOaN5JW8V5sk",
      "metadata": {
        "id": "wOaN5JW8V5sk"
      },
      "outputs": [],
      "source": [
        "#\n",
        "class TwitterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "v4aR-r9sVpVD",
      "metadata": {
        "id": "v4aR-r9sVpVD"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "validation_encodings = tokenizer(list(validation_texts), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
        "    \n",
        "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
        "validation_dataset = TwitterDataset(validation_encodings, validation_labels)\n",
        "test_dataset = TwitterDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jgba2oxgWMjf",
      "metadata": {
        "id": "jgba2oxgWMjf"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "#Load model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert_model/\", output_attentions=False)\n",
        "\n",
        "#Train model\n",
        "trainer = Trainer(\n",
        "            model=model,                         # the instantiated Transformers model to be trained\n",
        "            args=training_args,                  # training arguments\n",
        "            train_dataset=train_dataset,         # training dataset\n",
        "            eval_dataset=validation_dataset      # evaluation dataset\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4Nj7YZAWOqR",
      "metadata": {
        "id": "f4Nj7YZAWOqR"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "#return prediction  probabilities\n",
        "def model_predict(dataset):\n",
        "    logits = trainer.predict(dataset).predictions\n",
        "    probabilities = softmax(logits, axis = 1)\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T5fj0me-WXgX",
      "metadata": {
        "id": "T5fj0me-WXgX"
      },
      "outputs": [],
      "source": [
        "probabilities = model_predict(test_dataset)# the test dataset is from the original training data not the final testing dataset\n",
        "\n",
        "#Get predicted labels\n",
        "y_preds = []\n",
        "for i in probabilities:\n",
        "    y_preds.append(np.argmax(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i3c9t0sTZQad",
      "metadata": {
        "id": "i3c9t0sTZQad"
      },
      "outputs": [],
      "source": [
        "#Compute performance metrics\n",
        "\n",
        "f1_score = f1_score(test_labels, y_preds, average='macro')\n",
        "precision_score= precision_score(test_labels, y_preds, average='macro')\n",
        "recall_score = recall_score(test_labels, y_preds, average='macro')\n",
        "accuracy_score = accuracy_score(test_labels, y_preds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.x",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
